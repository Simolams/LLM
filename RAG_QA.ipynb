{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, HuggingFaceInstructEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import HuggingFaceHub\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from dotenv import load_dotenv\n",
    "import utils \n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = search_google('who is the composer of eine kleine nachtmusik')\n",
    "\n",
    "from langchain.schema import Document\n",
    "\n",
    "ds = [Document(page_content=text) for text in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "textes = ''\n",
    "\n",
    "for text in results : \n",
    "\n",
    "    textes+=text\n",
    "    textes+='\\n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector_store(text_chunks):\n",
    "    \n",
    "    \n",
    "    embeddings = HuggingFaceInstructEmbeddings(model_name = \"thenlper/gte-small\")\n",
    "\n",
    "    vectorstore = FAISS.from_texts(texts = text_chunks, embedding = embeddings)\n",
    "    \n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunk_text(text):\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    max_chunk_size = 150  # Maximum number of tokens per chunk\n",
    "    chunks = [] \n",
    "    current_chunk = []\n",
    "    current_chunk_length = 0\n",
    "\n",
    "    # Iterate through the tokens\n",
    "    for token in tokens:\n",
    "        # If adding the token to the current chunk doesn't exceed the max_chunk_size\n",
    "        if current_chunk_length + len(token) <= max_chunk_size:\n",
    "            current_chunk.append(token)\n",
    "            current_chunk_length += len(token)\n",
    "        else:\n",
    "            # If adding the token exceeds the limit, start a new chunk\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = [token]\n",
    "            current_chunk_length = len(token)\n",
    "\n",
    "    # Add the last chunk (if any)\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "text_chunks = get_chunk_text(textes)\n",
    "vectore_store = get_vector_store(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conversation_chain(vector_store):\n",
    "    \n",
    "\n",
    "    llm = HuggingFaceHub(repo_id=\"google/flan-t5-large\", model_kwargs={\"temperature\":0.5, \"max_length\":512},huggingfacehub_api_token = \"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\" )\n",
    "    memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm = llm,\n",
    "        retriever = vector_store.as_retriever(),\n",
    "        memory = memory\n",
    "    )\n",
    "\n",
    "    return conversation_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\llm\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'InferenceApi' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "conversation_chain = get_conversation_chain(vectore_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = conversation_chain.invoke({\"question\" : 'who is the composer of eine kleine nachtmusik'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'who is the composer of eine kleine nachtmusik',\n",
       " 'chat_history': [HumanMessage(content='who is the composer of eine kleine nachtmusik'),\n",
       "  AIMessage(content='Wolfgang Amadeus Mozart')],\n",
       " 'answer': 'Wolfgang Amadeus Mozart'}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\llm\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'InferenceApi' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFaceHub(repo_id=\"google/flan-t5-large\", model_kwargs={\"temperature\":0.5, \"max_length\":512},huggingfacehub_api_token = \"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\" )\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are stupid.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke('why you are stupid?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
