{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ab9456a-4fe9-4991-83b4-5fbd8588d957",
   "metadata": {},
   "source": [
    "# Projet Question Answering - Annexes\n",
    "\n",
    "Ne pas hésiter à retélécharger ce fichier régulièrement pour voir si le\n",
    "contenu a évolué.\n",
    "\n",
    "## Installation de pyserini\n",
    "\n",
    "L’installation est assez délicate, les instruction suivantes sont\n",
    "inspirées de\n",
    "<https://github.com/castorini/pyserini/blob/master/docs/installation.md>:\n",
    "\n",
    "``` shell\n",
    "conda create -n pyserini python=3.10 -y\n",
    "conda activate pyserini\n",
    "conda install -c conda-forge openjdk=11 maven -y\n",
    "conda install -c conda-forge lightgbm nmslib -y\n",
    "conda install -c pytorch faiss-cpu mkl=2021 blas=1.0=mkl pytorch -y\n",
    "pip install pyserini\n",
    "```\n",
    "\n",
    "Une version sans conda devrait marcher aussi, à condition d’avoir Python\n",
    "3.10 et pas une version plus récente:\n",
    "\n",
    "``` shell\n",
    "python -m venv serini\n",
    "source serini/bin/activate\n",
    "sudo apt update\n",
    "sudo apt install default-jdk build-essential\n",
    "pip install faiss-cpu\n",
    "pip install torch --index-url https://download.pytorch.org/whl/cu118\n",
    "pip install pyserini\n",
    "```\n",
    "\n",
    "## Installation des modules Huggingface\n",
    "\n",
    "``` shell\n",
    "pip install datasets\n",
    "pip install transformers\n",
    "```\n",
    "\n",
    "Ces deux bibliothèques gèrent le téléchargement des datasets et des\n",
    "modèles pré-entraînés. Le téléchargement se fait une fois pour toute\n",
    "lors de la première exécution.\n",
    "\n",
    "## Installation de Langchain\n",
    "\n",
    "``` shell\n",
    "pip install langchain\n",
    "```\n",
    "\n",
    "## Chargement du dataset `nq_open`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e393320-4421-4411-86b5-1c23e9d88ad5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m      3\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnq_open\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39miter(batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"nq_open\", split=\"validation\")\n",
    "for batch in dataset.iter(batch_size=1):\n",
    "    print(batch[\"question\"], batch[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dc05ff-3d04-495e-8dc4-a0558e143be5",
   "metadata": {},
   "source": [
    "## Modèle probabiliste avec Pyserini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59caa20e-58a7-46b3-83d9-0a7343ab03cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pyserini.search.lucene import LuceneSearcher\n",
    "\n",
    "searcher = LuceneSearcher.from_prebuilt_index('wikipedia-dpr-100w')\n",
    "hits = searcher.search('How old is Harrisson Ford ?')\n",
    "\n",
    "for i in range(0, 10):\n",
    "    docid = hits[i].docid\n",
    "    score = hits[i].score\n",
    "    content = json.loads(searcher.doc(docid).raw())[\"contents\"]\n",
    "\n",
    "    print(score, docid, content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab613464-42c7-4090-a0c6-c897e17ec14a",
   "metadata": {},
   "source": [
    "Cet index a été calculé à partir d’une extraction complète de wikipedia.\n",
    "Chaque article a été découpé en passages d’une longueur de 100 mots. Un\n",
    "document de l’index est l’un de ces passages.\n",
    "\n",
    "## Modèle dense avec Pyserini\n",
    "\n",
    "Ne pas l’utiliser tel quel, c’est trop coûteux pour une machine\n",
    "personnelle, ce fragment est juste donné à titre de référence. On voit\n",
    "ici l’encodeur de question\n",
    "`facebook/dpr-question_encoder-single-nq-base` et l’index précalculé\n",
    "avec l’encodeur de documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc47191-fa84-444a-b6b5-d8c7c02b6fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyserini.search.faiss import FaissSearcher, DprQueryEncoder\n",
    "# \n",
    "# encoder = DprQueryEncoder('facebook/dpr-question_encoder-single-nq-base')\n",
    "# searcher = FaissSearcher.from_prebuilt_index(\n",
    "#     'wikipedia-dpr-100w.dpr-single-nq',\n",
    "#     encoder\n",
    "# )\n",
    "# hits = searcher.search('what is a lobster roll')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765fb75f-9a9d-4d8b-bd65-c92f0a3a1bad",
   "metadata": {},
   "source": [
    "Il s’agit des mêmes documents que précédemment.\n",
    "\n",
    "## Modèle dense avec Fakesearch\n",
    "\n",
    "Il faut télécharger le fichier `fakesearch.py` et le fichier\n",
    "`faiss-validation.pickle`. Les résultats de recherche ont été\n",
    "précalculés et stockés dans un fichier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35926438-ec23-46c5-b2d9-6dfdb337890e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fakesearch\n",
    "\n",
    "dense_index = fakesearch.load(\"faiss-lucene.pickle\")\n",
    "dense_index[\"what is a lobster roll\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a2f37c-84ea-4116-b2aa-c1125a946a17",
   "metadata": {},
   "source": [
    "Attention, on ne peut utiliser que les questions du sous-ensemble de\n",
    "validation. On obtiendra une erreur dans le cas contraire:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420a5389-16bf-45c9-b056-b085009dd929",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_index[\"How old is Harrison Ford\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badea24f-40f3-4177-bc47-17a33daf7527",
   "metadata": {},
   "source": [
    "## Modèle probabiliste avec Fakesearch\n",
    "\n",
    "La recherche pré-calculée n’est à utiliser que si vous n’arrivez\n",
    "vraiment pas à installer `pyserini`.\n",
    "\n",
    "Il faut télécharger le fichier `fakesearch.py` et le fichier\n",
    "`lucence-validation.pickle`. Les résultats de recherche ont été\n",
    "précalculés et stockés dans un fichier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553f9470-8ef5-4ab8-a5bc-c4986983f237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fakesearch\n",
    "\n",
    "sparse_index = fakesearch.load(\"lucene-validation.pickle)\n",
    "sparse_index[\"what is a lobster roll\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a676504-e4f8-4359-8c13-5c2e15b64b23",
   "metadata": {},
   "source": [
    "Attention, on ne peut utiliser que les questions du sous-ensemble de\n",
    "validation. On obtiendra une erreur dans le cas contraire:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcb5710-42c5-487e-8915-57702a1d0ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_index[\"How old is Harrison Ford\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3693027-fd09-4251-a6bc-fd0a48f1a224",
   "metadata": {},
   "source": [
    "## ChatGPT avec le module OpenAI\n",
    "\n",
    "Cet interface est relativement simple, mais limitée à ChatGPT et ne\n",
    "fournit aucun outil pour faciliter l’utilisation des réponses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9d8cb1-9461-41f9-a9a8-4345e03e6e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=LA_CLÉ_ICI)\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Who is Joe Biden ?\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-3.5-turbo\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065a6555-0443-4f7b-964f-b751bdf61bc5",
   "metadata": {},
   "source": [
    "## ChatGPT avec Langchain\n",
    "\n",
    "Langchain est un peu plus complexe à utiliser, mais permet d’utiliser\n",
    "beaucoup plus d’outils pratiques et de s’abstraire du modèle de langue\n",
    "utilisé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad90eff7-0aa9-4752-8a44-cddbd19bc5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=LA_CLÉ_ICI)\n",
    "llm.invoke(\"what is a lobster roll\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b05d51-8ae0-44f0-a55f-c48fa851acd8",
   "metadata": {},
   "source": [
    "## Duckduckgo avec Langchain\n",
    "\n",
    "Voir <https://python.langchain.com/docs/integrations/tools/ddg>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a552031f-d3d3-42e2-9fa1-9fd8decb84c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet  duckduckgo-search\n",
    "\n",
    "from langchain.tools import DuckDuckGoSearchRun\n",
    "\n",
    "search = DuckDuckGoSearchRun()\n",
    "search.run(\"what is a lobster roll\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ef7554-834e-4045-9289-b5ffba360408",
   "metadata": {},
   "source": [
    "## Llama2\n",
    "\n",
    "Pour Llama2, on a besoin de:\n",
    "\n",
    "-   Ollama <https://ollama.ai/> pour télécharger et faire tourner le\n",
    "    modèle\n",
    "-   Langchain pour l’utiliser\n",
    "\n",
    "Voir <https://python.langchain.com/docs/integrations/chat/ollama>\n",
    "\n",
    "## GPT2\n",
    "\n",
    "Voir <https://huggingface.co/gpt2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232f5aeb-c120-4b87-b690-ceb99253f290",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "pipe(\"what is a lobster roll\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
