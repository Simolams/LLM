{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match Accuracy (BM25): 0.08\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pyserini.search.lucene import LuceneSearcher\n",
    "from transformers import pipeline, AutoModelForQuestionAnswering, AutoTokenizer\n",
    "\n",
    "# Load the NQ-Open dataset from Hugging Face\n",
    "splits = {\"validation\": \"nq_open/validation-00000-of-00001.parquet\"}  # Define the validation split file\n",
    "df_validation = pd.read_parquet(\"hf://datasets/google-research-datasets/nq_open/\" + splits[\"validation\"])\n",
    "\n",
    "# Extract questions and answers\n",
    "questions = df_validation[\"question\"].tolist()\n",
    "answers = df_validation[\"answer\"].tolist()\n",
    "\n",
    "# Load BM25 searcher\n",
    "searcher = LuceneSearcher.from_prebuilt_index('wikipedia-dpr-100w')\n",
    "\n",
    "# Load the extractive QA model\n",
    "qa_model_name = \"timpal0l/mdeberta-v3-base-squad2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(qa_model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(qa_model_name)\n",
    "qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "def clean_answer(answer):\n",
    "    \"\"\"Ensure the extracted answer is concise and properly formatted.\"\"\"\n",
    "    answer = answer.strip().split(\".\")[0]  # Remove long explanations\n",
    "    answer = answer.replace(\",\", \"\").strip()  # Remove commas\n",
    "    answer = \" \".join(answer.split()[:5])  # Limit to 5 words max\n",
    "    return answer\n",
    "\n",
    "def answer_question_bm25(question, top_k=20, min_score=0.0):\n",
    "    hits = searcher.search(question, k=top_k)\n",
    "    if not hits:\n",
    "        return \"No answer found\"\n",
    "    \n",
    "    # Extract and limit document context\n",
    "    best_doc = json.loads(searcher.doc(hits[0].docid).raw())[\"contents\"]\n",
    "    best_doc = \" \".join(best_doc.split()[:150])  # Keep only first 150 words\n",
    "\n",
    "    qa_input = {\"question\": question, \"context\": best_doc}\n",
    "    result = qa_pipeline(qa_input)\n",
    "    \n",
    "    # Apply confidence filtering\n",
    "    if result[\"score\"] < min_score:\n",
    "        return \"No answer found\"\n",
    "\n",
    "    return clean_answer(result[\"answer\"])\n",
    "\n",
    "\n",
    "def exact_match(pred, true):\n",
    "    return int(any(pred.strip().lower() == t.strip().lower() for t in true))\n",
    "\n",
    "# Evaluation function for BM25\n",
    "def evaluate_bm25(n_samples=50):\n",
    "    results = []\n",
    "    correct = 0\n",
    "    for i in range(n_samples):\n",
    "        pred = answer_question_bm25(questions[i])\n",
    "        is_correct = exact_match(pred, answers[i])\n",
    "        correct += is_correct\n",
    "        results.append({\n",
    "            \"question\": questions[i],\n",
    "            \"true_answers\": \", \".join(answers[i]),\n",
    "            \"predicted\": pred,\n",
    "            \"correct\": is_correct\n",
    "        })\n",
    "    accuracy = correct / n_samples\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results.to_csv(\"outputs/bm25_predictions.csv\", index=False)\n",
    "    return accuracy\n",
    "\n",
    "# Run BM25 evaluation\n",
    "accuracy_bm25 = evaluate_bm25(3600)\n",
    "print(f\"Exact Match Accuracy (BM25): {accuracy_bm25:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match Accuracy (BM25): 0.00\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import AIMessage\n",
    "from pyserini.search.lucene import LuceneSearcher\n",
    "from transformers import pipeline, AutoModelForQuestionAnswering, AutoTokenizer\n",
    "\n",
    "# Load the NQ-Open dataset from Hugging Face\n",
    "splits = {\"validation\": \"nq_open/validation-00000-of-00001.parquet\"}  # Define the validation split file\n",
    "df_validation = pd.read_parquet(\"hf://datasets/google-research-datasets/nq_open/\" + splits[\"validation\"])\n",
    "\n",
    "# Extract questions and answers\n",
    "questions = df_validation[\"question\"].tolist()\n",
    "answers = df_validation[\"answer\"].tolist()\n",
    "\n",
    "# Load BM25 searcher\n",
    "searcher = LuceneSearcher.from_prebuilt_index('wikipedia-dpr-100w')\n",
    "\n",
    "# Load the extractive QA model\n",
    "qa_model_name = \"timpal0l/mdeberta-v3-base-squad2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(qa_model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(qa_model_name)\n",
    "qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "def clean_answer(answer):\n",
    "    \"\"\"Clean the extracted answer by removing unnecessary punctuation and whitespace.\"\"\"\n",
    "    return answer.strip().split(\".\")[0] + \",\"\n",
    "\n",
    "def answer_question_bm25(question, top_k=5):\n",
    "    hits = searcher.search(question, k=top_k)\n",
    "    if not hits:\n",
    "        return \"No answer found\"\n",
    "    \n",
    "    # Extract best document\n",
    "    best_doc = json.loads(searcher.doc(hits[0].docid).raw())[\"contents\"]\n",
    "    qa_input = {\"question\": question, \"context\": best_doc}\n",
    "    result = qa_pipeline(qa_input)\n",
    "    return clean_answer(result[\"answer\"])\n",
    "\n",
    "def exact_match(pred, true):\n",
    "    return int(any(pred.strip().lower() == t.strip().lower() for t in true))\n",
    "\n",
    "# Evaluation function for BM25\n",
    "def evaluate_bm25(n_samples=50):\n",
    "    results = []\n",
    "    correct = 0\n",
    "    for i in range(n_samples):\n",
    "        pred = answer_question_bm25(questions[i])\n",
    "        is_correct = exact_match(pred, answers[i])\n",
    "        correct += is_correct\n",
    "        results.append({\n",
    "            \"question\": questions[i],\n",
    "            \"true_answers\": \", \".join(answers[i]),\n",
    "            \"predicted\": pred,\n",
    "            \"correct\": is_correct\n",
    "        })\n",
    "    accuracy = correct / n_samples\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results.to_csv(\"bm25_predictions.csv\", index=False)\n",
    "    return accuracy\n",
    "\n",
    "# Run BM25 evaluation\n",
    "accuracy_bm25 = evaluate_bm25(10)\n",
    "print(f\"Exact Match Accuracy (BM25): {accuracy_bm25:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyserini",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
